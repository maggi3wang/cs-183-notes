\subsection{The Statistical Learning Framework}

Learner's input:
\begin{itemize}
    \item \textbf{Domain set}: Set $\mathcal{X}$ that we wish to label. Represented by a vector of features. Domain points: instances, $\mathcal{X}$: instance space.
    \item \textbf{Label set}: Set $\mathcal{Y}$ of possible labels
    \item \textbf{Training data}: $S = ((x_1, y_1) \dots (x_m, y_m))$, finite sequence of pairs in $\mathcal{X} \cross \mathcal{Y}$. Training examples / training set.
    \item \textbf{The learner's output}: prediction rule, $h : \mathcal{X} \rightarrow \mathcal{Y}$. Predictor, hypothesis, classifier.
    \item \textbf{A simple data-generation model}: each pair in the training data $S$ is generated by sampling a point $x_i$ according to $\mathcal{D}$ (probability distribution over $\mathcal{X}$ by $\mathcal{D}$) and then labeling it by $f$.
    \item \textbf{Measure of success}: error of a prediction rule, $h : \mathcal{X} \rightarrow \mathcal{Y}$ is the probability of randomly choosing an ex. $x$ for which $h(x) \neq f(x)$:
    $$L_{\mathcal{D}, f}(h) = \mathbb{P}_{x \sim \mathcal{D}} [h(x) \neq f(x)] = \mathcal{D}(\{x:h(x) \neq f(x)\})$$
    Generalization error, the risk, the true error of $h$.

\end{itemize}

\subsection{Empirical Risk Minimization}

\textbf{Training error} / empirical error / empirical risk - error the classifier incurs over the training sample: $$L_S(h) = \frac{|\{i \in [m] : h(x_i) \neq y_i \}|}{m}$$

\textbf{Empirical Risk Minimization (ERM)}: coming up with a predictor $h$ that minimizes $L_S(h)$.

\subsubsection{Overfitting}

\textbf{Overfitting}: $h$ fits training data ``too well"

\begin{equation*}
    h_S(x) = 
    \begin{cases}
        y_i & \text{if } \exists i \in [m] \text{ s.t. } x_i = x \\
        0 & \text{otherwise.}
    \end{cases}
\end{equation*}

\subsection{Empirical Risk Minimization with Inductive Bias}

Apply ERM over a restricted search space (\textbf{hypothesis class} $\mathcal{H}$), thus biasing it towards a particular set of predictors. Such restrictions are called an \textbf{inductive bias} - ideally based on prior knowledge of problem.

$$\textrm{ERM}_{\mathcal{H}} (S) \in \argmin_{h \in \mathcal{H}} L_s(h)$$

Tradeoff - more restricted hypothesis class better protects from overfitting but causes stronger inductive bias.

\subsubsection{Finite hypothesis classes}

If $\mathcal{H}$ is a finite class then $\text{ERM}_{\mathcal{H}}$ will not overfit, provided it is based on a sufficiently large training sample.

Let $h_S$ denote a result of applying $\text{ERM}_H$ to $S$,
$$h_S \in \argmin_{h \in \mathcal{H}} L_S(h)$$

\textbf{Definition 2.1: The Realizability Assumption}

There exists $h^* \in \mathcal{H}$ s.t. $L_{(\mathcal{D}, f)}(h^*) = 0$.

This assumption implies that with probability 1 over random samples, $S$, where the instances are sampled according to $D$ and are labeled by $f$, we have $L_S(h^*) = 0$.

\bigskip

\textbf{The i.i.d. assumption:} $S \sim \mathcal{D}^m$, where $m$ is the size of $S$, and $\mathcal{D}^m$ denotes the probability over $m$-tuples induced by applying $\mathcal{D}$ to pick each element of the tuple independently of the other members of the tuple.

\smallskip

\hrule height 0.06pt

\smallskip

$\delta$ is probability of getting a non-representative sample, and $(1-\delta)$ is the confidence parameter of our prediction.

$\epsilon$ is the accuracy parameter. Event $L_{(\mathcal{D}, f)}(h_S) > \epsilon$ is failure of the learner, while if $L_{(\mathcal{D}, f)}(h_S) \leq \epsilon$ the output of the algorithm is an approximately correct predictor.

\textbf{Corollary 2.3:}

Let $\mathcal{H}$ be a finite hypothesis class. Let $\delta \in (0, 1)$ and $\epsilon > 0$ and let $m$ be an integer that satisfies $m \geq \frac{\log(|\mathcal{H}| \delta)}{\epsilon}$.

Then, for any labeling function, $f$, and for any distribution, $\mathcal{D}$, for which the realizability assumption holds (that is, for some $h \in \mathcal{H}, L_{(\mathcal{D}, f)}(h) = 0)$ with probability of at least $1-\delta$ over the choice of an i.i.d. sample $S$ of size $m$, we have that for every ERM hypothesis, $h_S$, it holds that 
    $$L_{(\mathcal{D}, f)}(h_S) \leq \epsilon$$

\medskip

For a sufficiently large $m$, the $\text{ERM}_\mathcal{H}$ rule over a finite hypothesis will be \textit{probably} (with confidence $1-\delta$) \textit{approximately} (up to an error of $\epsilon$) correct.

\medskip

\textbf{Proof:}

Let $S|_x = (x_1, \dots, x_m)$ be the instances of the training set.

We would like to upper bound $\mathcal{D}^m({S|_x : L_{(\mathcal{D}, f)}(h_S) > \epsilon})$.

Set of ``bad" hypotheses: $\mathcal{H}_B = \{ h \in \mathcal{H} : L_{(\mathcal{D}, f)}(h) > \epsilon \}$.

Set of misleading examples: $M = \{S|_x : \exists h \in \mathcal{H}_B, L_S(h) = 0\}$.

For every $S|_x \in M$, there is a ``bad" hypothesis, $h \in \mathcal{H}_B$ that looks like a ``good" hypothesis on $S|_x$.

The event $L_{(\mathcal{D}, f)} (h_S) > \epsilon$ can only happen if our sample is in the set of misleading samples, $M$: 

$$\{ S |_x : L_{(\mathcal{D}, f)} (h_S) > \epsilon \} \subseteq M$$

We can rewrite $M$ as $M = \bigcup_{h \in \mathcal{H}_B} \{S |_x : L_S(h) = 0\}$.

$\mathcal{D}^m(\{S |_x : L_{(\mathcal{D}, f)} (h_S) > \epsilon \}) \leq \mathcal{D}^m (M) = \mathcal{D}^m (\cup_{h \in \mathcal{H}_B} \{S |_x : L_S(h) = 0 \})$.

Upper bound right-hand side using union bound.

\bigskip

\textbf{Lemma 2.2: Union Bound}

For any two sets $A$, $B$ and a distribution $\mathcal{D}$ we have
    $$\mathcal{D}(A \cup B) \leq \mathcal{D}(A) + \mathcal{D}(B)$$

$$\mathcal{D}^m(\{S |_x : L_{(\mathcal{D}, f)} (h_S) > \epsilon) \leq \sum_{h \in \mathcal{H}_B} \mathcal{D}^m ( \{ S|_x : L_S(h) = 0 \})$$

\begin{equation*}
    \begin{aligned}
        \mathcal{D}^m (\{S |_x : L_S(h) = 0 \}) 
        &= \mathcal{D}^m(\{S|_x : \forall i, h(x_i) = f(x_i) \}) \\
        &= \prod_{i=1}^m \mathcal{D}(\{x_i : h(x_i) = f(x_i) \})
    \end{aligned}
\end{equation*}

For each individual sampling of an element of the training set,
$$\mathcal{D}(\{x_i : h(x_i) = y_i\}) = 1 - L_{(\mathcal{D}, f)}(h) \leq 1 - \epsilon$$

Using $1 - \epsilon \leq e^{-\epsilon}$, for every $h \in \mathcal{H}_B$,
$$\mathcal{D}^m(\{S|_x : L_S(h) = 0 \}) \leq (1-\epsilon)^m \leq e^{-\epsilon m}$$

We conclude that 
$$\mathcal{D}^m(\{S|_x : L_{(\mathcal{D}, f)}(h_S) > \epsilon \}) \leq |\mathcal{H}_B|^{-\epsilon m} \leq |\mathcal{H}| e^{-\epsilon m}$$