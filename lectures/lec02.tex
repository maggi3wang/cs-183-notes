\subsection{PAC learnability}

A hypothesis class $\mathcal{H}$ is \textbf{PAC learnable} if there exists a function $m_{\mathcal{H}} : (0, 1)^2 \rightarrow \mathbb{N}$ and a learning algorithm with the following property: For every $\epsilon, \delta \in (0, 1)$, for every distribution $\mathcal{D}$ over $\mathcal{X}$, and for every labeling function $f : \mathcal{X} \rightarrow \{0, 1\}$, if the realizability assumption holds w.r.t. $\mathcal{H}, \mathcal{D}, f$, then when running the learning algorithm on $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. examples generated by $\mathcal{D}$ and labeled by $f$, the algorithm returns a hypothesis $h$ s.t. w.p. of at least $1 - \delta$ (over the choice of the examples), $L_{(\mathcal{D}, f)}(h) \leq \epsilon$.

$$\mathcal{P}[L_{(\mathcal{D}, f)}(h) > \epsilon] < \delta \Longleftrightarrow \mathcal{P}[L_{(\mathcal{D}, f)}(h) < \epsilon] > 1 - \delta$$

$\epsilon$: accuracy parameter, determines how far the output classifier can be from the optimal one (``approximately correct").

$\delta$: confidence parameter, how likely the classifier is to meet the accuracy requirement (``probably").

\medskip

\hrule height 0.06pt

$m_{\mathcal{H}}: (0, 1)^2 \rightarrow \mathbb{N}$ determines the \textbf{sample complexity} of learning $\mathcal{H}$. Function of $\epsilon$ and $\delta$, and depends on properties of $\mathcal{H}$.

Minimal function: for any $\epsilon, \delta$, $m_{\mathcal{H}}(\epsilon, \delta)$ is the minimal integer that satisfies the requirements of PAC learning with accuracy $\epsilon$ and confidence $\delta$.

\textbf{Corollary 3.2:} Every finite hypothesis class is PAC learnable with sample complexity
$$m_{\mathcal{H}} (\epsilon, \delta) \leq \left\lceil \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} \right\rceil$$

Later, we'll see that what determines PAC learnability of a class is not its finiteness but its VC dimension.

\subsection{The Bayes Optimal Predictor}

Given any probability distribution $\mathcal{D}$ over $\mathcal{X} \times \{0, 1\}$, the best label predicting function from $\mathcal{X}$ to $\{0, 1\}$ will be

\begin{equation*}
    f_{\mathcal{D}}(x) = 
    \begin{cases}
    1 & \text{if } \mathbb{P}[y = 1 | x] \geq 1/2 \\
    0 & \text{otherwise}
    \end{cases}
\end{equation*}

For every probability distribution $\mathcal{D}$, the Bayes optimal predictor $f_D$ is optimal, in the sense that no other classifier, $g : \mathcal{X} \rightarrow \{0, 1\}$, has a lower error. For every classifier $g$, $L_{\mathcal{D}}(f_{\mathcal{D}}) \leq L_{\mathcal{D}}(g)$.

Since we do not know $\mathcal{D}$, we cannot utilize this optimal predictor $f_{\mathcal{D}}$.

\subsection{Agnostic PAC learnability}

Generalization on 1. removing the realizability assumption (diff goal), 2. handle feature labeled in multiple ways, 3. generalized loss functions.

\subsubsection{1. Removing the realizability assumption}

$$L_{\mathcal{D}}(h) \leq \min_{h' \in \mathcal{H}} L_D(h') + \epsilon$$

With agnostic PAC learning, learner can still declare success if its error is not much larger than the best error achievable by a predictor from the class $\mathcal{H}$.

\subsubsection{2. Handle feature labeled in multiple ways}

Relax realizability assumption by replacing target labeling function w data-labels generating distribution. $\mathcal{D}$ is a joint probability distribution over $\mathcal{X} \times \mathcal{Y}$. Marginal distribution $\mathcal{D}_x$ and conditional distribution $\mathcal{D}((x, y)|x)$. Allows for two papayas that share the same color and hardness to belong to different taste categories.

Redefine true error of prediction rule $h$ to be 
$$L_{\mathcal{D}}(h) = \mathbb{P}_{(x, y) \sim \mathcal{D}}[h(x) \neq y] = \mathcal{D}(\{(x,y): h(x) \neq y\}).$$

\subsubsection{3. Beyond binary classification}

\textbf{Multiclass classification}

\textbf{Regression}

\subsubsection{Generalized loss functions}

\textbf{Loss functions}: given any set $\mathcal{H}$ and some domain $Z$ let $l$ be any function from $\mathcal{H} \times Z$ to the set of nonnegative real numbers, $l: \mathcal{H} \times Z \rightarrow \mathbb{R}_+$. For prediction problems $Z = \mathcal{X} \times \mathcal{Y}$.

\textbf{Risk function}: expected loss of a classifier, $h \in \mathcal{H}$, w.r.t. a probability distribution $\mathcal{D}$ over $Z$, 
$$L_{\mathcal{D}(h)} = \mathbb{E}_{z \sim \mathcal{D}}[l(h, z)]$$

We consider the expectation of the loss of $h$ over objects $z$ picked randomly according to $\mathcal{D}$.

\textbf{Empirical risk}: expected loss over a given sample $S = (z_1, \dots, z_m) \in Z^m$,
$$L_S(h) = \frac{1}{m} \sum_{i=1}^m l(h, z_i).$$

\textbf{0-1 loss}: r.v. $z$ ranges over the set of pairs $\mathcal{X} \times \mathcal{Y}$ and
\begin{equation*}
    l_{0-1}(h, (x, y)) = 
    \begin{cases}
        0 & \text{if } h(x) = y \\
        1 & \text{if } h(x) \neq y
    \end{cases}
\end{equation*}

\textbf{Square loss}: r.v. $z$ ranges over the set of pairs $\mathcal{X} \times \mathcal{Y}$ and
$$l_{\text{sq}}(h, (x, y)) = (h(x) - y)^2$$

\hrule height 0.06pt
\medskip

\textbf{Def 3.4: Agnostic PAC learnability for general loss fns}

A hypothesis class $\mathcal{H}$ is agnostic PAC learnable w.r.t. a set $Z$ and a loss function $l: \mathcal{H} \times Z \rightarrow \mathbb{R}_+$, if there exists a function $m_{\mathcal{H}}: (0, 1)^2 \rightarrow \mathbb{N}$ and a learning algorithm with the following property: For every $\epsilon, \delta \in (0, 1)$ and for every distribution $\mathcal{D}$ over $\mathcal{X} \times \mathcal{Y}$, when running the learning algorithm on $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. examples generated by $\mathcal{D}$, the algorithm returns a hypothesis $h \in \mathcal{H}$ s.t., w.p. of at least $1 - \delta$ (over the choice of the $m$ training examples), 
$$L_{\mathcal{D}}(h) \leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') + \epsilon,$$
where $L_{\mathcal{D}}(h) = \mathbb{E}_{z \sim \mathcal{D}}[l(h, z)]$.

\hrule height 0.06pt
\medskip

\subsection{Learning via uniform convergence}

We need that uniformly over all hypotheses in the hypothesis class, the empirical risk will be close to the true risk.

\textbf{Def 4.1: $\mathbf{\epsilon}$-representative sample}

A training set $S$ is called $\epsilon$-representative (w.r.t. domain $Z$, hypothesis class $\mathcal{H}$, loss function $l$, and distribution $\mathcal{D}$) if 
$$\forall h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon.$$

\textbf{Lemma 4.2:}

Assume that a training set $S$ is $\frac{\epsilon}{2}$-representative (w.r.t. domain $Z$, hypothesis class $\mathcal{H}$, loss function $l$, and distribution $\mathcal{D}$). Then, any output of $\textrm{ERM}_{\mathcal{H}(S)}$, namely, any $h_S \in \argmin_{h \in \mathcal{H}} L_S(h)$, satisfies
    $$L_{\mathcal{D}}(h_S) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \epsilon$$
    
\textit{Proof.} For every $h \in \mathcal{H}$, 
$$L_{\mathcal{D}}(h_S) \leq L_S(h_S) + \frac{\epsilon}{2} \leq L_S(h) + \frac{\epsilon}{2} \leq L_{\mathcal{D}}(h) + \frac{\epsilon}{2} + \frac{\epsilon}{2} = L_{\mathcal{D}}(h) + \epsilon$$

Follows from assumption that $S$ is $\frac{\epsilon}{2}$-representative and the second inequality holds since $h_S$ is an ERM predictor.

\textbf{Def 4.3: Uniform Convergence}

We say that a hypothesis class $\mathcal{H}$ has the uniform convergence property (w.r.t. a domain $Z$ and a loss function $l$) if there exists a function $m_{\mathcal{H}}^{\text{UC}}: (0, 1)^2 \rightarrow \mathbb{N}$ such that for every $\epsilon, \delta \in (0, 1)$ and for every probability distribution $\mathcal{D}$ over $Z$, if $S$ is a sample of $m \geq m_{\mathcal{H}}^{\text{UC}}(\epsilon, \delta)$ examples drawn i.i.d. according to $\mathcal{D}$, then, w.p. of at least $1 - \delta$, $S$ is $\epsilon$-representative.

\textbf{Corollary 4.4:}

If a class $\mathcal{H}$ has the uniform convergence property with a fnction $m_{\mathcal{H}}^{\text{UC}}$ then the class is agnostically PAC learnable with the sample complexity $m_{\mathcal{H}}(\epsilon, \delta) \leq m_{\mathcal{H}}^{\text{UC}}(\epsilon/2, \delta)$. Furthermore, in that case, the $\textrm{ERM}_{\mathcal{H}}$ paradigm is a successful agnostic PAC learner for $\mathcal{H}$.

\subsection{Finite classes are agnostic PAC learnable}

Uniform convergence holds for a finite hypothesis class, so agnostic PAC learnable.

Step 1: Apply the union bound

Fix some $\epsilon, \delta$. We need to find a sample size $m$ that guarantees that
$$\mathcal{D}^m(\{S : \forall h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon \}) \geq 1 - \delta.$$

Equivalently,
$$\mathcal{D}^m(\{S : \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}) < \delta.$$

Writing
$$\{S : \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \} = \bigcup_{h \in \mathcal{H}} \{S : |L_S(h) - L_{\mathcal{D}}(h) > \epsilon \},$$

and applying the union bound, we obtain (4.1)
$$\mathcal{D}^m(\{S : \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}) = \sum_{h \in \mathcal{H}} \mathcal{D}^m \{S : |L_S(h) - L_{\mathcal{D}}(h) > \epsilon \}.$$

Step 2: Employ a measure concentration inequality. Each summand of the RHS is small enough for a sufficiently large $m$.

Law of large numbers: when $m$ goes to $\infty$, empirical averages converge to their true expectation. However, only an asympotic result. Measure concentration inequality, quantifies gap btwn empirical averages and their expected value:

\textbf{Lemma 4.5: Hoeffding's Inequality}

Let $\theta_1, \dots, \theta_m$ be a sequence i.i.d. random variables and assume that for all $i$, $\mathcal{E}[\theta_i] = \mu$ and $\mathcal{P}[a \leq \theta_i \leq b] = 1$. Then, for any $\epsilon > 0$, 

$$\mathbb{P}[|\frac{1}{m} \sum_{i = 1}^m \theta_i - \mu| > \epsilon] \leq 2 \exp(-2m\epsilon^2 / (b-a)^2)$$

\medskip

Let $\theta_i$ be the r.v. $l(h, z_i)$. Since $h$ is fixed and $z_1, \dots, z_m$ are sampled i.i.d., it follows that $\theta_1, \dots, \theta_m$ are also i.i.d. r.v.s. Furthermore, $L_S(h) = \frac{1}{m} \sum_{i=1}^m l(h, z_i) = \frac{1}{m} \sum_{i=1}^m \theta_i$ and $L_{\mathcal{D}}(h) = \mu$. Assume $l \in [0, 1]$, so $\theta_i \in [0, 1]$. We obtain

$$\mathcal{D}^m(\{S :| L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}) = \mathbb{P}[|\frac{1}{m} \sum_{i=1}^m \theta_i - \mu | > \epsilon] \leq 2 \exp (-2m\epsilon^2).$$

Combining w eq (4.1) yields
\begin{align*}
\mathcal{D}^m(\{S:\exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon\}) 
&\leq \sum_{h \in \mathcal{H}} 2 \exp(-2m\epsilon^2) \\
&= 2|\mathcal{H}| \exp(-2m\epsilon^2)
\end{align*}

Finally, if we choose
$$m \geq \frac{\log(2 |\mathcal{H}|/ \delta)}{2\epsilon^2}$$
then
$$\mathcal{D}^m(\{S: \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h) | > \epsilon \}) \leq \delta.$$

\textbf{Corollary 4.6:}

Let $\mathcal{H}$ be a finite hypothesis class, let $Z$ be a domain, and let $l : \mathcal{H} \times Z \rightarrow [0, 1]$ be a loss function. Then, $\mathcal{H}$ enjoys the uniform convergence property with sample complexity 
$$m_{\mathcal{H}}^{UC}(\epsilon, \delta) \leq \left\lceil \frac{\log(2 |\mathcal{H}|/\delta)}{2 \epsilon^2} \right\rceil.$$

Furthermore, the class is agnostically PAC learnable using the ERM algorithm with sample complexity
$$m_{\mathcal{H}}(\epsilon, \delta) \leq m_{\mathcal{H}}^{UC}(\epsilon/2, \delta) \leq \left\lceil \frac{2 \log(2 |\mathcal{H}|/\delta)}{\epsilon^2} \right\rceil.$$

\newpage

\subsection{The bias-complexity trade-off}

\subsubsection{The no-free-lunch theorem}

We prove there is no universal learner--no learner can succeed on all learning tasks.

\textbf{Theorem 5.1: No-free-lunch}: Let $A$ be any learning algorithm for the task of binary classification w.r.t. the 0-1 loss over a domain $\mathcal{X}$. Let $m$ be any number smaller than $|\mathcal{X}|/2$, representing a training set size. Then, there exists a distribution $\mathcal{D}$ over $\mathcal{X} \times \{0, 1 \}$ s.t.:

1. There exists a fn $f : \mathcal{X} \rightarrow \{0, 1\}$ with $L_{\mathcal{D}}(f) = 0$.

2. W.p. of at least $1/7$ over the choice of $S \sim \mathcal{D}^m$ we have that $L_{\mathcal{D}}(A(S)) \geq 1/8$.

\textit{Proof}. Let $C$ be a subset of $\mathcal{X}$ of size $2m$. The intuition of the proof is that any learning algorithm that observes only half of the instances in $C$ has no information on what should be the labels of the rest of the instances in $C$.

\bigskip

\textbf{Corollary 5.2:}

Let $\mathcal{X}$ be an infinite domain set and let $\mathcal{H}$ be the set of all functions from $\mathcal{X}$ to $\{0, 1\}$. Then, $\mathcal{H}$ is not PAC learnable.

\subsection{Error decomposition}

Let $h_S$ be an $\text{ERM}_{\mathcal{H}}$ hypothesis. Then,
$$L_{\mathcal{D}}(h_S) = \epsilon_{\text{app}} + \epsilon_{\text{est}}, \epsilon_{\text{app}} = \min_{h \in \mathcal{H}} L_{\mathcal{D}} (h), \epsilon_{\text{est}} = L_{\mathcal{D}}(h_S) - \epsilon_{\text{app}}.$$

\textbf{Approximation error}: measures how much risk we have bc we strict ourselves to a specific class -- how much inductive bias we have

\textbf{The estimation error}: difference between approx error and error achieved, results bc empirical risk is only an estimate of the true risk

\bigskip

\textbf{Bias-complexity tradeoff}: more bias, overfitting. Less bias, underfitting.